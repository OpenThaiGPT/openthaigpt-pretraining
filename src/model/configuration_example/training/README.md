# Config
- accelerator: device (cuda , cpu)
- strategy: mode of parallel (dp | ddp | ddp_spawn | xla | deepspeed | fsdp)
- stage: stage of deepspeed
- offload_optimizer: true when want to offload optimizer
- offload_parameters: true when want to offload parameters
- gradient_clipping: gradient clip of deepspeed strategy
- num_gpus: number of gpus which each node use
- precision: data type (32-true | 32 | 16-mixed | bf16-mixed | 64-true) 
- num_nodes: number of node
- seed: seed of this training (13, 21, 42, 87, 100)
- batch_size: batch size of model
- grad: gradient accumulation steps
- max_tokens: max sequence lenght
- num_shards: number chunk of dataset
- num_workers: number of worker in dataloader
- epochs: numbers of epochs
- start_epochs: start epoch for resume training
- start_steps: start steps of start epochs
- save_steps: save every n steps
- eval_steps: eval every n steps
- save_paths: path to save model
- load_weight_path: path to load weight of model
- decay_lr: when true will use learning rate scheduler
- warmup_iters: number of warmup iteration
- lr_decay_iters: number iteration to activate learning rate scheduler
- min_lr: minimun of learning rate