name: llama_hf
pretrained_model_name_or_path: null
args:
  vocab_size: 32000
  hidden_size: 1024 #4096
  intermediate_size: 11008
  num_hidden_layers: 8 #32
  num_attention_heads: 8 #32
  hidden_act: "silu"
  max_position_embeddings: 2048
  initializer_range: 0.02
  rms_norm_eps: 1e-6
  use_cache: True
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
  tie_word_embeddings: False
  use_checkpointing: True
  checkpoint_only_attention: False
  lora: False
tokenizer:
  pretrained_model_name_or_path: decapoda-research/llama-7b-hf
  tokenizer_class: LlamaTokenizer
lora:
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  lora_bias: "none"
