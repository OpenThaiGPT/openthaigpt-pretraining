name: llama_hf
pretrained_model_name_or_path: decapoda-research/llama-7b-hf
args:
  vocab_size: 32000
  hidden_size: 4096
  intermediate_size: 11008
  num_hidden_layers: 32
  num_attention_heads: 32
  hidden_act: "silu"
  max_position_embeddings: 2048
  initializer_range: 0.02
  rms_norm_eps: 1e-6
  use_cache: True
  pad_token_id: 0
  bos_token_id: 1
  eos_token_id: 2
  tie_word_embeddings: False
  use_checkpointing: True
  checkpoint_only_attention: False
tokenizer:
  pretrained_model_name_or_path: decapoda-research/llama-7b-hf
  tokenizer_class: LlamaTokenizer
train_dataset:
  dataset_name: oscar
  split: train  # type: ignore
  shuffle: false
  buffer_size: 10000
  streaming: false
  from_disk: false
val_dataset:
  dataset_name: oscar
  split: validation  # type: ignore
  shuffle: false
  buffer_size: 10000
  streaming: false
  from_disk: false