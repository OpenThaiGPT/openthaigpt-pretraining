name: gptj
pretrained_model_name_or_path: EleutherAI/gpt-j-6b
args:
  vocab_size: 50400
  n_positions: 2048
  n_embd: 4096
  n_layer: 28
  n_head: 16
  rotary_dim: 64
  n_inner: 16384 #4 times n_embd
  activation_function: "gelu_new"
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  attn_pdrop: 0.0
  layer_norm_epsilon: 1e-5
  initializer_range: 0.02
  use_cache: True
  bos_token_id: 50256
  eos_token_id: 50256
  tie_word_embeddings: False
  use_checkpointing: True
  checkpoint_only_attention: False
  attention_mode: xformers
tokenizer:
  pretrained_model_name_or_path: EleutherAI/gpt-j-6b
  tokenizer_class: AutoTokenizer
optimizer:
  name: adamw
  hyps:
    lr: 1e-4
    weight_decay: 1e-2
    betas: [0.9, 0.95]
    fused: true