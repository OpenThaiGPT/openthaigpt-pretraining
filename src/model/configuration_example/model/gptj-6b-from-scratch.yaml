name: gptj
pretrained_model_name_or_path: null
args:
  vocab_size: 50400
  n_positions: 2048
  n_embd: 1024
  n_layer: 16
  n_head: 16
  rotary_dim: 64
  n_inner: 4096 #4 times n_embd
  activation_function: "gelu_new"
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  attn_pdrop: 0.0
  layer_norm_epsilon: 1e-5
  initializer_range: 0.02
  use_cache: True
  bos_token_id: 50256
  eos_token_id: 50256
  tie_word_embeddings: False
  use_checkpointing: True
  checkpoint_only_attention: True
  attention_mode: xformers #xformers
  lora: False
tokenizer:
  pretrained_model_name_or_path: EleutherAI/gpt-j-6b
  tokenizer_class: AutoTokenizer
lora:
  lora_r: 8
  lora_alpha: 32
  lora_dropout: 0.05
  lora_bias: "none"
