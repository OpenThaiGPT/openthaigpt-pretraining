name: gptj
pretrained_model_name_or_path: null
args:
  vocab_size: 50400
  n_positions: 2048
  n_embd: 1024
  n_layer: 16
  n_head: 16
  rotary_dim: 64
  n_inner: 4096 #4 times n_embd
  activation_function: "gelu_new"
  resid_pdrop: 0.0
  embd_pdrop: 0.0
  attn_pdrop: 0.0
  layer_norm_epsilon: 1e-5
  initializer_range: 0.02
  use_cache: True
  bos_token_id: 50256
  eos_token_id: 50256
  tie_word_embeddings: False
  use_checkpointing: True
  checkpoint_only_attention: True
  attention_mode: xformers #xformers
tokenizer:
  pretrained_model_name_or_path: EleutherAI/gpt-j-6b
  tokenizer_class: AutoTokenizer
tokenized_dataset: None
train_dataset:
  dataset_name: oscar
  split: train  # type: ignore
  shuffle: false
  buffer_size: 10000
  streaming: false
  from_disk: false
val_dataset:
  dataset_name: oscar
  split: validation  # type: ignore
  shuffle: false
  buffer_size: 10000
  streaming: false
  from_disk: false