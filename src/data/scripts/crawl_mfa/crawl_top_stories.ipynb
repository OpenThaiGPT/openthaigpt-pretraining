{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root = 'https://www.mfa.go.th'\n",
    "top_stories_url = 'https://www.mfa.go.th/th/page/%E0%B8%82%E0%B9%88%E0%B8%B2%E0%B8%A7%E0%B9%80%E0%B8%94%E0%B9%88%E0%B8%99?menu=5d5bd3d815e39c306002aac4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_title_date(cur_url, page_no):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        get titles and dates for news in every pafe\n",
    "    Args:\n",
    "        desired url to be used as a root and a total number of pages.\n",
    "    Returns:\n",
    "        news_list contains titles and dates\n",
    "        \"\"\"\n",
    "    page = 1\n",
    "    news_list = []\n",
    "    unwanted_classes = [('div','d-inline-block'),('div','pt-3 col')]\n",
    "\n",
    "    while page != page_no+1:\n",
    "      url = f'{cur_url}&p={page}'\n",
    "      res = requests.get(url)\n",
    "      res.encoding = \"utf-8\"\n",
    "      soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "      info = soup.find_all('div', class_='p-3 col-md-4')\n",
    "      date_list = soup.find_all('p', class_='date')\n",
    "\n",
    "      # Exclude unrelated data \n",
    "      for inf in info:\n",
    "          for tag_name, class_attributes in unwanted_classes:\n",
    "                unwanted_data = soup.find_all(tag_name, class_=class_attributes)\n",
    "                for data in unwanted_data:\n",
    "                  data.extract()\n",
    "          # Get news titles\n",
    "          title = inf.get_text(strip=True, separator=' ')\n",
    "          # Get new dates\n",
    "          for indiv_date in date_list:\n",
    "            indiv_date = indiv_date.get_text(strip=True, separator=' ')\n",
    "\n",
    "          news_dict = {'title': title, 'date': indiv_date}\n",
    "          news_list.append(news_dict)\n",
    "\n",
    "      page = page + 1\n",
    "\n",
    "\n",
    "    return news_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_info(cur_url, page_no):\n",
    "    \"\"\"\n",
    "    Description:\n",
    "        get data inside a link for every pafe\n",
    "    Args:\n",
    "        desired url and total of pages.\n",
    "    Returns:\n",
    "        info_list contains details of the news\n",
    "        \"\"\"\n",
    "    page = 1\n",
    "    info_list = []\n",
    "    href_list = []\n",
    "\n",
    "    while page != page_no+1:\n",
    "      url = f'{cur_url}&p={page}'\n",
    "      res = requests.get(url)\n",
    "      res.encoding = \"utf-8\"\n",
    "      soup = BeautifulSoup(res.content, 'lxml')\n",
    "\n",
    "      info = soup.find_all('div', class_='p-3 col-md-4')\n",
    "\n",
    "      for branch in info:\n",
    "        link = branch.find('a')\n",
    "        if link:\n",
    "          href_list.append(link['href'])\n",
    "\n",
    "      for href in href_list:\n",
    "        result = requests.get(f'{root}{href}')\n",
    "        content = result.text\n",
    "        soup = BeautifulSoup(content, 'lxml')   \n",
    "\n",
    "        details = soup.find_all('div', class_='ContentDetailstyled__ContentDescription-sc-150bmwg-4 jWrYsI mb-3')\n",
    "        for element in details:\n",
    "          detail = element.get_text(strip=True, separator=' ')\n",
    "          info_list.append(detail)       \n",
    "\n",
    "      page = page + 1\n",
    "\n",
    "    return info_list\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_title_date = get_title_date(top_stories_url,216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_details = get_info(top_stories_url,216)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data_dict in enumerate(news_title_date):\n",
    "    if i < len(news_details):\n",
    "        data_dict.update({'detail': news_details[i]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_news = pd.DataFrame(news_title_date)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
