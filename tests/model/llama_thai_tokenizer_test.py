# import time
from openthaigpt_pretraining_model.llama_thai_tokenizer.tokenizer import (
    LLaMaToken,
    EngThaiLLaMaToken,
)

TEXT_TEST_CASES = [
    "การใช้งานหลักของ LLaMA คือการวิจัยเกี่ยวกับรูปแบบภาษาที่ใหญ่",
    "LLaMA มุ่งเน้นที่การศึกษารูปแบบภาษาที่กว้างขวาง",
    "อยากให้วันนี้ pull request ผ่าน",
    "ขอเพิ่มสัก 1 point ก็ยังดีครับ",
    "reviewer ใจดีมากกกกกกกกกกกกกก",
]

LABEL_TEST_ENGTHAI_CASES = [
    [
        "▁การใช้งาน",
        "หลักของ",
        "▁L",
        "La",
        "MA",
        "▁คือการ",
        "วิจัย",
        "เกี่ยวกับ",
        "รูปแบบ",
        "ภาษา",
        "ที่ใหญ่",
    ],
    [
        "▁L",
        "La",
        "MA",
        "▁มุ่ง",
        "เน้น",
        "ที่การ",
        "ศึกษา",
        "รูปแบบ",
        "ภาษา",
        "ที่",
        "กว้างขวาง",
    ],
    ["▁อยากให้", "วันนี้", "▁pull", "▁request", "▁ผ่าน"],
    ["▁ขอ", "เพิ่ม", "สัก", "▁1", "▁point", "▁ก็ยัง", "ดีครับ"],
    ["▁rev", "iewer", "▁ใจ", "ดีมาก", "กกกก", "กกกก", "กกกก", "ก"],
]

LABEL_TEST_LLAMA_CASES = [
    [
        "▁",
        "ก",
        "า",
        "ร",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "ช",
        "้",
        "ง",
        "า",
        "น",
        "ห",
        "ล",
        "ั",
        "ก",
        "ข",
        "อ",
        "ง",
        "▁L",
        "La",
        "MA",
        "▁",
        "ค",
        "ื",
        "อ",
        "ก",
        "า",
        "ร",
        "ว",
        "ิ",
        "จ",
        "ั",
        "ย",
        "เ",
        "ก",
        "ี",
        "่",
        "ย",
        "ว",
        "ก",
        "ั",
        "บ",
        "ร",
        "ู",
        "ป",
        "แ",
        "บ",
        "บ",
        "ภ",
        "า",
        "ษ",
        "า",
        "ท",
        "ี",
        "่",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "ห",
        "ญ",
        "่",
    ],
    [
        "▁L",
        "La",
        "MA",
        "▁",
        "ม",
        "ุ",
        "่",
        "ง",
        "เ",
        "น",
        "้",
        "น",
        "ท",
        "ี",
        "่",
        "ก",
        "า",
        "ร",
        "ศ",
        "<0xE0>",
        "<0xB8>",
        "<0xB6>",
        "ก",
        "ษ",
        "า",
        "ร",
        "ู",
        "ป",
        "แ",
        "บ",
        "บ",
        "ภ",
        "า",
        "ษ",
        "า",
        "ท",
        "ี",
        "่",
        "ก",
        "ว",
        "้",
        "า",
        "ง",
        "ข",
        "ว",
        "า",
        "ง",
    ],
    [
        "▁",
        "อ",
        "ย",
        "า",
        "ก",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "ห",
        "้",
        "ว",
        "ั",
        "น",
        "น",
        "ี",
        "้",
        "▁pull",
        "▁request",
        "▁",
        "<0xE0>",
        "<0xB8>",
        "<0x9C>",
        "่",
        "า",
        "น",
    ],
    [
        "▁",
        "ข",
        "อ",
        "เ",
        "พ",
        "ิ",
        "่",
        "ม",
        "ส",
        "ั",
        "ก",
        "▁",
        "1",
        "▁point",
        "▁",
        "ก",
        "็",
        "ย",
        "ั",
        "ง",
        "ด",
        "ี",
        "ค",
        "ร",
        "ั",
        "บ",
    ],
    [
        "▁rev",
        "iewer",
        "▁",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "จ",
        "ด",
        "ี",
        "ม",
        "า",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
    ],
]

TEXT_TEST_ENGLISH = "Convert Pretrained LLaMa to Token"


def test_merge_tokenizer():
    engthai_tokenizer = EngThaiLLaMaToken()
    for idx, test_text in enumerate(TEXT_TEST_CASES):
        assert engthai_tokenizer.tokenize(test_text) == LABEL_TEST_ENGTHAI_CASES[idx]


def test_llama_tokenizer():
    llama_tokenizer = LLaMaToken()
    for idx, test_text in enumerate(TEXT_TEST_CASES):
        assert llama_tokenizer.tokenize(test_text) == LABEL_TEST_LLAMA_CASES[idx]


def test_encode_english():
    engthai_tokenizer = EngThaiLLaMaToken()
    llama_tokenizer = LLaMaToken()
    assert engthai_tokenizer.encode(TEXT_TEST_ENGLISH) == llama_tokenizer.encode(
        TEXT_TEST_ENGLISH
    )
