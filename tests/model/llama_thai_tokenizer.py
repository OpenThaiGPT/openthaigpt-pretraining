# import time
from openthaigpt_pretraining_model.llama_thai_tokenizer.tokenizer import (
    LLaMaToken,
    EngThaiLLaMaToken,
)

TEXT_TEST_CASES = [
    "การใช้งานหลักของ LLaMA คือการวิจัยเกี่ยวกับรูปแบบภาษาที่ใหญ่",
    "LLaMA มุ่งเน้นที่การศึกษารูปแบบภาษาที่กว้างขวาง",
    "อยากให้วันนี้ pull request ผ่าน",
    "ขอเพิ่มสัก 1 point ก็ยังดีครับ",
    "reviewer ใจดีมากกกกกกกกกกกกกก",
]

LABEL_TEST_ENGTHAI_CASES = [
    [
        "▁",
        "ก",
        "า",
        "ร",
        "ใช้",
        "ง",
        "า",
        "น",
        "หล",
        "ั",
        "ก",
        "ของ",
        "▁L",
        "La",
        "MA",
        "▁คือ",
        "ก",
        "า",
        "ร",
        "วิจัย",
        "เกี่ยวกับ",
        "รูปแบบ",
        "ภ",
        "า",
        "ษ",
        "า",
        "ที่",
        "ให",
        "ญ",
        "่",
    ],
    [
        "▁L",
        "La",
        "MA",
        "▁",
        "มุ่งเน้น",
        "ที่",
        "ก",
        "า",
        "รศ",
        "ึก",
        "ษ",
        "า",
        "รูปแบบ",
        "ภ",
        "า",
        "ษ",
        "า",
        "ที่",
        "กว",
        "้",
        "า",
        "ง",
        "ขวาง",
    ],
    [
        "▁อ",
        "ย",
        "า",
        "ก",
        "ให้",
        "วันนี้",
        "▁pull",
        "▁request",
        "▁",
        "ผ",
        "่",
        "า",
        "น",
    ],
    [
        "▁ขอ",
        "เพิ่ม",
        "ส",
        "ั",
        "ก",
        "▁",
        "1",
        "▁point",
        "▁ก็",
        "ยัง",
        "ดี",
        "คร",
        "ั",
        "บ",
    ],
    ["▁rev", "iewer", "▁", "ใจดี", "มากกก", "กก", "กก", "กก", "กก", "กก", "ก"],
]

LABEL_TEST_LLAMA_CASES = [
    [
        "▁",
        "ก",
        "า",
        "ร",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "ช",
        "้",
        "ง",
        "า",
        "น",
        "ห",
        "ล",
        "ั",
        "ก",
        "ข",
        "อ",
        "ง",
        "▁L",
        "La",
        "MA",
        "▁",
        "ค",
        "ื",
        "อ",
        "ก",
        "า",
        "ร",
        "ว",
        "ิ",
        "จ",
        "ั",
        "ย",
        "เ",
        "ก",
        "ี",
        "่",
        "ย",
        "ว",
        "ก",
        "ั",
        "บ",
        "ร",
        "ู",
        "ป",
        "แ",
        "บ",
        "บ",
        "ภ",
        "า",
        "ษ",
        "า",
        "ท",
        "ี",
        "่",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "ห",
        "ญ",
        "่",
    ],
    [
        "▁L",
        "La",
        "MA",
        "▁",
        "ม",
        "ุ",
        "่",
        "ง",
        "เ",
        "น",
        "้",
        "น",
        "ท",
        "ี",
        "่",
        "ก",
        "า",
        "ร",
        "ศ",
        "<0xE0>",
        "<0xB8>",
        "<0xB6>",
        "ก",
        "ษ",
        "า",
        "ร",
        "ู",
        "ป",
        "แ",
        "บ",
        "บ",
        "ภ",
        "า",
        "ษ",
        "า",
        "ท",
        "ี",
        "่",
        "ก",
        "ว",
        "้",
        "า",
        "ง",
        "ข",
        "ว",
        "า",
        "ง",
    ],
    [
        "▁",
        "อ",
        "ย",
        "า",
        "ก",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "ห",
        "้",
        "ว",
        "ั",
        "น",
        "น",
        "ี",
        "้",
        "▁pull",
        "▁request",
        "▁",
        "<0xE0>",
        "<0xB8>",
        "<0x9C>",
        "่",
        "า",
        "น",
    ],
    [
        "▁",
        "ข",
        "อ",
        "เ",
        "พ",
        "ิ",
        "่",
        "ม",
        "ส",
        "ั",
        "ก",
        "▁",
        "1",
        "▁point",
        "▁",
        "ก",
        "็",
        "ย",
        "ั",
        "ง",
        "ด",
        "ี",
        "ค",
        "ร",
        "ั",
        "บ",
    ],
    [
        "▁rev",
        "iewer",
        "▁",
        "<0xE0>",
        "<0xB9>",
        "<0x83>",
        "จ",
        "ด",
        "ี",
        "ม",
        "า",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
        "ก",
    ],
]


def test_merge_tokenizer():
    engthai_tokenizer = EngThaiLLaMaToken()
    for idx, test_text in enumerate(TEXT_TEST_CASES):
        assert engthai_tokenizer.tokenize(test_text) == LABEL_TEST_ENGTHAI_CASES[idx]


def test_llama_tokenizer():
    llama_tokenizer = LLaMaToken()
    for idx, test_text in enumerate(TEXT_TEST_CASES):
        assert llama_tokenizer.tokenize(test_text) == LABEL_TEST_LLAMA_CASES[idx]
